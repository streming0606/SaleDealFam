#!/usr/bin/env python3
import logging
import re
import os
import json
import asyncio
import time
from datetime import datetime, timedelta
import pytz
import requests
from telegram import Update, Bot
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from telegram.error import TelegramError
import traceback

# Set up logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# Get configuration from environment variables
BOT_TOKEN = os.environ.get('BOT_TOKEN')
CHANNEL_ID = os.environ.get('CHANNEL_ID')
AMAZON_AFFILIATE_TAG = os.environ.get('AMAZON_TAG')
SESSION_TYPE = os.environ.get('SESSION_TYPE', 'morning')
WEBSITE_REPO = "streming0606/DealFamSheduler"
GITHUB_TOKEN = os.environ.get('PERSONAL_ACCESS_TOKEN')
SERP_API_KEY = os.environ.get('SERP_API_KEY')

# Session configuration
SESSION_CONFIG = {
    'morning': {
        'telegram_links': 3, 
        'website_links': 2, 
        'time': '10:12-10:20 AM'
    },
    'afternoon': {
        'telegram_links': 3, 
        'website_links': 2, 
        'time': '1:12-1:20 PM'
    },
    'evening': {
        'telegram_links': 2, 
        'website_links': 1, 
        'time': '6:12-6:20 PM'
    },
    'night': {
        'telegram_links': 2, 
        'website_links': 1, 
        'time': '9:12-9:20 PM'
    }
}

class EnhancedAffiliateBot:
    def __init__(self):
        self.bot = Bot(token=BOT_TOKEN)
        self.links = self.load_amazon_links()
        self.current_index = self.load_progress()
        self.serp_request_count = 0
        self.serp_start_time = datetime.now()
        
        logger.info(f"üöÄ Enhanced Affiliate Bot Initialized")
        logger.info(f"üìä Loaded {len(self.links)} total links")
        logger.info(f"üìç Starting from index: {self.current_index}")
        logger.info(f"üîë SerpApi key: {'Yes (***' + SERP_API_KEY[-4:] + ')' if SERP_API_KEY else 'No'}")
        
        # Test SerpApi connection on initialization
        if SERP_API_KEY:
            asyncio.create_task(self.test_serpapi_connection())
    
    async def test_serpapi_connection(self):
        """Test SerpApi connection with correct parameters"""
        try:
            logger.info("üß™ Testing SerpApi connection...")
            
            # Test with a search query first
            params = {
                'engine': 'amazon',
                'amazon_domain': 'amazon.in',
                'k': 'B08N5WRWNW',  # Use 'k' parameter as per SerpApi docs
                'api_key': SERP_API_KEY
            }
            
            response = requests.get('https://serpapi.com/search', params=params, timeout=20)
            
            logger.info(f"üß™ Test response status: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                logger.info(f"üß™ Test response keys: {list(data.keys())}")
                
                if 'error' in data:
                    logger.error(f"üß™ SerpApi error: {data['error']}")
                    # Log the full error for debugging
                    logger.error(f"üß™ Full response: {json.dumps(data, indent=2)[:1000]}...")
                    return False
                else:
                    logger.info("‚úÖ SerpApi connection test successful!")
                    
                    # Check organic results
                    if 'organic_results' in data and data['organic_results']:
                        first_result = data['organic_results'][0]
                        logger.info(f"üß™ Sample title: {first_result.get('title', 'No title')[:50]}...")
                        logger.info(f"üß™ Sample price: {first_result.get('price', 'No price')}")
                        logger.info(f"üß™ Sample ASIN: {first_result.get('asin', 'No ASIN')}")
                    
                    return True
            else:
                logger.error(f"üß™ Test failed with status: {response.status_code}")
                logger.error(f"üß™ Response: {response.text[:500]}...")
                return False
                
        except Exception as e:
            logger.error(f"üß™ Test exception: {e}")
            logger.error(f"üß™ Full traceback: {traceback.format_exc()}")
            return False
        
    def load_amazon_links(self):
        """Load Amazon links from file"""
        try:
            os.makedirs('data', exist_ok=True)
            
            if os.path.exists('data/amazon_links.json'):
                with open('data/amazon_links.json', 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    links = data.get('links', [])
                    logger.info(f"‚úÖ Loaded {len(links)} links from amazon_links.json")
                    return links
                    
        except Exception as e:
            logger.error(f"‚ùå Error loading links: {e}")
        
        logger.warning("‚ö†Ô∏è No links loaded - returning empty list")
        return []
    
    def load_progress(self):
        """Load current progress"""
        try:
            # Try GitHub first
            github_progress = self.get_progress_from_github()
            if github_progress is not None:
                logger.info(f"üìà Loaded progress from GitHub: index {github_progress}")
                return github_progress
            
            # Try local file
            if os.path.exists('data/progress.json'):
                with open('data/progress.json', 'r') as f:
                    data = json.load(f)
                    index = data.get('current_index', 0)
                    logger.info(f"üìà Loaded progress from local file: index {index}")
                    return index
                    
        except Exception as e:
            logger.error(f"‚ùå Error loading progress: {e}")
        
        logger.info("üìà No progress found - starting from index 0")
        return 0
    
    def get_progress_from_github(self):
        """Get current progress from GitHub repository"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                if not GITHUB_TOKEN:
                    return None
                    
                url = f"https://api.github.com/repos/{WEBSITE_REPO}/contents/data/progress.json"
                headers = {
                    'Authorization': f'token {GITHUB_TOKEN}',
                    'Accept': 'application/vnd.github.v3+json'
                }
                
                response = requests.get(url, headers=headers, timeout=15)
                
                if response.status_code == 200:
                    file_content = response.json()
                    import base64
                    content = base64.b64decode(file_content['content']).decode('utf-8')
                    data = json.loads(content)
                    return data.get('current_index', 0)
                elif response.status_code == 404:
                    logger.info("No progress file found in GitHub repository")
                    return None
                else:
                    logger.warning(f"GitHub API returned {response.status_code}: {response.text}")
                    
            except Exception as e:
                logger.error(f"Attempt {attempt + 1}: Error getting progress from GitHub: {e}")
                
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
        
        return None
    
    def save_progress(self):
        """Save current progress"""
        try:
            os.makedirs('data', exist_ok=True)
            
            if not self.links:
                cycle_number, position_in_cycle = 1, 1
            else:
                total_links = len(self.links)
                cycle_number = (self.current_index // total_links) + 1
                position_in_cycle = (self.current_index % total_links) + 1
                
                if self.current_index > 0 and self.current_index % total_links == 0:
                    cycle_number = (self.current_index // total_links)
                    position_in_cycle = total_links
            
            ist_now = datetime.now(pytz.timezone('Asia/Kolkata'))
            
            progress_data = {
                'current_index': self.current_index,
                'cycle_number': cycle_number,
                'position_in_cycle': position_in_cycle,
                'total_links': len(self.links),
                'last_updated': ist_now.isoformat(),
                'session_type': SESSION_TYPE
            }
            
            # Save locally
            with open('data/progress.json', 'w') as f:
                json.dump(progress_data, f, indent=2)
            
            # Save to GitHub asynchronously
            asyncio.create_task(self.update_progress_on_github(progress_data))
            
            logger.info(f"üíæ Progress saved: Index {self.current_index}, Cycle {cycle_number}, Position {position_in_cycle}/{len(self.links)}")
                
        except Exception as e:
            logger.error(f"‚ùå Error saving progress: {e}")
    
    async def update_progress_on_github(self, progress_data):
        """Update progress file on GitHub"""
        try:
            if not GITHUB_TOKEN:
                return False
            
            success = await self.commit_to_github(
                'data/progress.json', 
                json.dumps(progress_data, indent=2)
            )
            
            if success:
                logger.info("‚úÖ Progress updated on GitHub repository")
            else:
                logger.error("‚ùå Failed to update progress on GitHub")
            
            return success
            
        except Exception as e:
            logger.error(f"‚ùå Error updating progress on GitHub: {e}")
            return False
    
    def convert_amazon_link(self, url):
        """Convert Amazon link to include affiliate tag"""
        try:
            # Remove existing affiliate tags
            url = re.sub(r'[?&]tag=[^&]*', '', url)
            
            # Extract ASIN and create clean link
            asin_match = re.search(r'/dp/([A-Z0-9]{10})', url)
            if asin_match:
                asin = asin_match.group(1)
                return f"https://www.amazon.in/dp/{asin}?tag={AMAZON_AFFILIATE_TAG}"
            
            # Fallback: append tag
            separator = '&' if '?' in url else '?'
            return f"{url}{separator}tag={AMAZON_AFFILIATE_TAG}"
            
        except Exception as e:
            logger.error(f"‚ùå Error converting link: {e}")
            return url
    
    def extract_asin_from_url(self, amazon_url):
        """Extract ASIN from Amazon URL"""
        try:
            asin_match = re.search(r'/dp/([A-Z0-9]{10})', amazon_url)
            if asin_match:
                return asin_match.group(1)
            return None
        except Exception as e:
            logger.error(f"‚ùå Error extracting ASIN: {e}")
            return None
    
    async def get_real_product_info_serpapi(self, asin):
        """Get real product information using SerpApi - COMPLETELY FIXED VERSION"""
        if not SERP_API_KEY:
            logger.warning("‚ùå No SerpApi key - using fallback product info")
            return self.get_fallback_product_info(asin)
        
        if not asin:
            logger.warning("‚ùå No ASIN provided - using fallback")
            return self.get_fallback_product_info(asin)
        
        try:
            logger.info(f"üîç Fetching real product data for ASIN: {asin}")
            
            # Rate limiting check
            current_time = datetime.now()
            if (current_time - self.serp_start_time).seconds < 3600:
                if self.serp_request_count >= 240:
                    logger.warning("‚ö†Ô∏è SerpApi rate limit approaching - using fallback")
                    return self.get_fallback_product_info(asin)
            else:
                self.serp_request_count = 0
                self.serp_start_time = current_time
            
            # CORRECTED: Use proper SerpApi parameters as per documentation
            params = {
                'engine': 'amazon',
                'amazon_domain': 'amazon.in',
                'k': asin,  # FIXED: Use 'k' parameter for search query (as per SerpApi docs)
                'api_key': SERP_API_KEY
            }
            
            logger.info(f"üì° Making SerpApi request for ASIN: {asin} (Request #{self.serp_request_count + 1})")
            
            response = requests.get('https://serpapi.com/search', params=params, timeout=30)
            
            logger.info(f"üì® SerpApi response status: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                
                # DEBUG: Log the actual response structure
                logger.info(f"üìä SerpApi response keys: {list(data.keys())}")
                
                # Check for API errors first
                if 'error' in data:
                    logger.error(f"‚ùå SerpApi API error: {data['error']}")
                    logger.error(f"üìã Full error response: {json.dumps(data, indent=2)[:800]}...")
                    return self.get_fallback_product_info(asin)
                
                # Look for organic_results (this is where Amazon product data appears)
                organic_results = data.get('organic_results', [])
                
                if not organic_results:
                    logger.warning(f"‚ö†Ô∏è No organic_results found for ASIN: {asin}")
                    logger.info(f"üìã Available data keys: {list(data.keys())}")
                    logger.info(f"üìã Raw response preview: {json.dumps(data, indent=2)[:1000]}...")
                    return self.get_fallback_product_info(asin)
                
                # Find the product in organic results
                product_result = None
                
                # Method 1: Look for exact ASIN match
                for result in organic_results:
                    result_asin = result.get('asin')
                    if result_asin == asin:
                        product_result = result
                        logger.info(f"‚úÖ Found exact ASIN match: {asin}")
                        break
                
                # Method 2: Look for ASIN in link
                if not product_result:
                    for result in organic_results:
                        result_link = result.get('link', '')
                        if asin in result_link:
                            product_result = result
                            logger.info(f"‚úÖ Found ASIN in product link: {asin}")
                            break
                
                # Method 3: Use first result (when searching by ASIN, first result is usually the match)
                if not product_result and organic_results:
                    product_result = organic_results[0]
                    logger.info(f"‚úÖ Using first organic result")
                
                if not product_result:
                    logger.warning(f"‚ùå No suitable product result found for ASIN: {asin}")
                    return self.get_fallback_product_info(asin)
                
                # Extract product information with detailed logging
                title = product_result.get('title', f'Amazon Product {asin}')
                price = self.extract_price_from_result(product_result)
                rating = self.extract_rating_from_result(product_result)
                thumbnail = product_result.get('thumbnail', '')
                
                # Log extracted data for debugging
                logger.info(f"üìù Extracted data:")
                logger.info(f"   üìù Title: {title}")
                logger.info(f"   üí∞ Price: {price}")
                logger.info(f"   ‚≠ê Rating: {rating}")
                logger.info(f"   üñºÔ∏è Thumbnail: {'Yes' if thumbnail else 'No'}")
                
                product_info = {
                    'asin': asin,
                    'title': title[:100] + '...' if len(title) > 100 else title,
                    'image': thumbnail or f"https://m.media-amazon.com/images/I/{asin}._AC_SL1500_.jpg",
                    'price': price,
                    'rating': rating,
                    'category': self.categorize_by_title(title),
                    'source': 'serpapi'
                }
                
                # Increment request counter
                self.serp_request_count += 1
                
                logger.info(f"üéâ SUCCESS! Real product data extracted from SerpApi!")
                logger.info(f"   üìä API calls used: {self.serp_request_count}/250")
                
                return product_info
                    
            else:
                logger.error(f"‚ùå SerpApi HTTP error: {response.status_code}")
                logger.error(f"üìã Response body: {response.text[:500]}...")
                
        except Exception as e:
            logger.error(f"‚ùå Exception in SerpApi call: {str(e)}")
            logger.error(f"üìã Full traceback: {traceback.format_exc()}")
        
        logger.warning(f"üîÑ Falling back to default for ASIN: {asin}")
        return self.get_fallback_product_info(asin)
    
    def extract_price_from_result(self, result):
        """Extract price from SerpApi result with enhanced methods"""
        try:
            # Method 1: Direct price field
            price = result.get('price')
            if price:
                if isinstance(price, str) and price.strip():
                    price_clean = price.strip()
                    logger.info(f"üí∞ Found direct price: {price_clean}")
                    return price_clean if '‚Çπ' in price_clean else f"‚Çπ{price_clean}"
                elif isinstance(price, dict):
                    current_price = price.get('current_price') or price.get('value') or price.get('from')
                    if current_price:
                        logger.info(f"üí∞ Found price from dict: {current_price}")
                        return current_price if '‚Çπ' in str(current_price) else f"‚Çπ{current_price}"
            
            # Method 2: Look in snippet for price patterns
            snippet = result.get('snippet', '')
            if snippet:
                price_patterns = [
                    r'‚Çπ[\d,]+(?:\.\d{2})?',  # ‚Çπ1,200 or ‚Çπ1,200.50
                    r'Rs\.?\s*[\d,]+',       # Rs 1200 or Rs. 1200
                    r'INR\s*[\d,]+',         # INR 1200
                    r'Price:\s*‚Çπ[\d,]+',     # Price: ‚Çπ1200
                ]
                
                for pattern in price_patterns:
                    price_match = re.search(pattern, snippet)
                    if price_match:
                        found_price = price_match.group()
                        logger.info(f"üí∞ Found price in snippet: {found_price}")
                        return found_price if '‚Çπ' in found_price else f"‚Çπ{found_price.replace('Rs.', '').replace('Rs', '').replace('INR', '').replace('Price:', '').strip()}"
            
            # Method 3: Check title for price
            title = result.get('title', '')
            if title:
                price_match = re.search(r'‚Çπ[\d,]+', title)
                if price_match:
                    found_price = price_match.group()
                    logger.info(f"üí∞ Found price in title: {found_price}")
                    return found_price
            
            logger.info(f"üí∞ No price found, using default")
                    
        except Exception as e:
            logger.error(f"‚ùå Error extracting price: {e}")
        
        return "‚ÇπSpecial Price"
    
    def extract_rating_from_result(self, result):
        """Extract rating from SerpApi result with enhanced methods"""
        try:
            # Method 1: Direct rating field
            rating = result.get('rating')
            if rating:
                if isinstance(rating, (int, float)):
                    stars = min(int(rating), 5)
                    rating_str = "‚≠ê" * stars + "‚òÜ" * (5-stars) if stars < 5 else "‚≠ê" * 5
                    logger.info(f"‚≠ê Found direct rating: {rating} -> {rating_str}")
                    return rating_str
                elif isinstance(rating, str):
                    rating_match = re.search(r'(\d+(?:\.\d+)?)', rating)
                    if rating_match:
                        rating_num = float(rating_match.group(1))
                        stars = min(int(rating_num), 5)
                        rating_str = "‚≠ê" * stars + "‚òÜ" * (5-stars) if stars < 5 else "‚≠ê" * 5
                        logger.info(f"‚≠ê Found rating from string: {rating} -> {rating_str}")
                        return rating_str
            
            # Method 2: Look in snippet for rating patterns
            snippet = result.get('snippet', '')
            if snippet:
                rating_patterns = [
                    r'(\d+(?:\.\d+)?)\s*(?:out of|\/)\s*5',  # 4.5 out of 5
                    r'(\d+(?:\.\d+)?)\s*stars?',             # 4.5 stars
                    r'Rating:\s*(\d+(?:\.\d+)?)',            # Rating: 4.5
                ]
                
                for pattern in rating_patterns:
                    rating_match = re.search(pattern, snippet)
                    if rating_match:
                        rating_num = float(rating_match.group(1))
                        stars = min(int(rating_num), 5)
                        rating_str = "‚≠ê" * stars + "‚òÜ" * (5-stars) if stars < 5 else "‚≠ê" * 5
                        logger.info(f"‚≠ê Found rating in snippet: {rating_num} -> {rating_str}")
                        return rating_str
            
            logger.info(f"‚≠ê No rating found, using default")
                        
        except Exception as e:
            logger.error(f"‚ùå Error extracting rating: {e}")
        
        return "‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ"
    
    def get_fallback_product_info(self, asin):
        """Enhanced fallback product info with realistic data"""
        fallback_titles = [
            f"Premium Electronics Deal - {asin}" if asin else "Premium Electronics Deal",
            f"Top Rated Amazon Product - {asin}" if asin else "Top Rated Amazon Product", 
            f"Trending Deal of the Day - {asin}" if asin else "Trending Deal of the Day",
            f"Best Seller Item - {asin}" if asin else "Best Seller Item",
            f"Featured Amazon Deal - {asin}" if asin else "Featured Amazon Deal"
        ]
        
        import random
        title = random.choice(fallback_titles)
        
        logger.info(f"üîÑ Using fallback data for ASIN: {asin}")
        
        return {
            'asin': asin or "UNKNOWN",
            'title': title,
            'image': f"https://m.media-amazon.com/images/I/{asin}._AC_SL1500_.jpg" if asin else "",
            'price': "‚ÇπSpecial Price",
            'rating': "‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ",
            'category': "electronics",
            'source': 'fallback'
        }
    
    def categorize_by_title(self, title):
        """Categorize product based on title"""
        title_lower = title.lower()
        
        categories = {
            'electronics': ['phone', 'smartphone', 'mobile', 'laptop', 'computer', 'tablet', 'earphone', 'headphone', 'charger', 'cable', 'speaker'],
            'fashion': ['shirt', 'tshirt', 'jeans', 'dress', 'shoes', 'watch', 'bag', 'clothing', 'jacket', 'cap'],
            'home': ['kitchen', 'furniture', 'home', 'decor', 'appliance', 'bedsheet', 'pillow', 'chair', 'table', 'lamp'],
            'health': ['skincare', 'beauty', 'cosmetic', 'health', 'supplement', 'medicine', 'cream', 'oil', 'soap']
        }
        
        for category, keywords in categories.items():
            if any(keyword in title_lower for keyword in keywords):
                return category
                
        return 'electronics'
    
    async def update_website_products(self, new_products):
        """Update website products.json file via GitHub API"""
        try:
            if not GITHUB_TOKEN:
                logger.warning("No GitHub token provided - skipping website update")
                return False
            
            website_products = await self.get_website_products()
            
            # Add new products to the beginning
            for product in reversed(new_products):
                website_products.insert(0, product)
            
            # Keep only latest 50 products
            website_products = website_products[:50]
            
            updated_data = {
                "last_updated": datetime.now(pytz.timezone('Asia/Kolkata')).isoformat(),
                "total_products": len(website_products),
                "daily_target": 6,
                "products": website_products
            }
            
            success = await self.commit_to_github('data/products.json', json.dumps(updated_data, indent=2))
            
            if success:
                logger.info(f"‚úÖ Website updated with {len(new_products)} new products")
                return True
            else:
                logger.error("‚ùå Failed to update website")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Error updating website: {e}")
            return False
    
    async def get_website_products(self):
        """Get existing products from website repository"""
        try:
            url = f"https://api.github.com/repos/{WEBSITE_REPO}/contents/data/products.json"
            headers = {
                'Authorization': f'token {GITHUB_TOKEN}',
                'Accept': 'application/vnd.github.v3+json'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                file_content = response.json()
                import base64
                content = base64.b64decode(file_content['content']).decode('utf-8')
                data = json.loads(content)
                return data.get('products', [])
            else:
                logger.info("No existing products file found - creating new one")
                return []
                
        except Exception as e:
            logger.error(f"‚ùå Error getting website products: {e}")
            return []
    
    async def commit_to_github(self, file_path, content):
        """Commit file to GitHub repository"""
        try:
            url = f"https://api.github.com/repos/{WEBSITE_REPO}/contents/{file_path}"
            headers = {
                'Authorization': f'token {GITHUB_TOKEN}',
                'Accept': 'application/vnd.github.v3+json'
            }
            
            # Get current file SHA if exists
            response = requests.get(url, headers=headers, timeout=10)
            sha = None
            if response.status_code == 200:
                sha = response.json()['sha']
            
            import base64
            encoded_content = base64.b64encode(content.encode('utf-8')).decode('utf-8')
            
            commit_data = {
                'message': f'Auto-update: {SESSION_TYPE} session - Index {self.current_index}',
                'content': encoded_content,
                'branch': 'main'
            }
            
            if sha:
                commit_data['sha'] = sha
            
            response = requests.put(url, headers=headers, json=commit_data, timeout=30)
            
            if response.status_code in [200, 201]:
                logger.info(f"‚úÖ Successfully committed {file_path}")
                return True
            else:
                logger.error(f"‚ùå GitHub commit failed: {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Error committing to GitHub: {e}")
            return False
    
    def get_next_links(self, count, purpose="general"):
        """Get next batch of links for scheduling"""
        if not self.links:
            logger.warning("‚ö†Ô∏è No links available for scheduling")
            return []
        
        selected_links = []
        
        logger.info(f"üìã Getting {count} links for {purpose} starting from index {self.current_index}")
        
        for i in range(count):
            if self.current_index >= len(self.links):
                self.current_index = 0
                logger.info(f"üîÑ Cycle completed! Wrapping around to index 0")
            
            selected_link = self.links[self.current_index]
            selected_links.append(selected_link)
            
            logger.info(f"   üìé Link {i+1}/{count}: Index {self.current_index} - {selected_link[:50]}...")
            self.current_index += 1
        
        # Save progress after getting links
        self.save_progress()
        
        logger.info(f"‚úÖ Selected {len(selected_links)} links for {purpose}. Next index: {self.current_index}")
        return selected_links
    
    async def send_scheduled_links(self, session_type):
        """Send links to Telegram and update Website"""
        config = SESSION_CONFIG.get(session_type, SESSION_CONFIG['morning'])
        telegram_count = config['telegram_links']
        website_count = config['website_links']
        time_slot = config['time']
        
        logger.info(f"üöÄ Starting {session_type} session: {time_slot} IST")
        logger.info(f"üì± Telegram posts: {telegram_count}")
        logger.info(f"üåê Website posts: {website_count}")
        logger.info(f"üìç Starting from index: {self.current_index}")
        
        # Get links for Telegram
        telegram_links = self.get_next_links(telegram_count, "telegram")
        # Get separate links for website
        website_links = self.get_next_links(website_count, "website")
        
        if not telegram_links and not website_links:
            logger.error("‚ùå No links available to send")
            return 0
        
        sent_count = 0
        website_products = []
        
        # Send to Telegram
        logger.info("üì± Sending to Telegram...")
        for i, original_link in enumerate(telegram_links, 1):
            try:
                converted_link = self.convert_amazon_link(original_link)
                
                channel_message = f"""üî• DEAL FAM ALERT! üî•

üõí Amazon Link: {converted_link}

üëç Deal Fam Rating: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
üí∞ Grab this amazing deal now!
‚è∞ Limited Time: 6 hours left!

#DealFam #AmazonDeals #FlipkartOffers #ShoppingDeals #IndianDeals #SaveMoney #DailyDeals"""
                
                await self.bot.send_message(
                    chat_id=CHANNEL_ID,
                    text=channel_message,
                    parse_mode='Markdown'
                )
                
                ist_time = datetime.now(pytz.timezone('Asia/Kolkata')).strftime("%I:%M:%S %p")
                logger.info(f"‚úÖ Telegram {i}/{telegram_count} sent at {ist_time} IST")
                
                sent_count += 1
                await asyncio.sleep(4)
                    
            except Exception as e:
                logger.error(f"‚ùå Error sending to Telegram {i}: {e}")
        
        # Process Website Products with SerpApi
        logger.info("üåê Processing website products with SerpApi...")
        for i, original_link in enumerate(website_links, 1):
            try:
                converted_link = self.convert_amazon_link(original_link)
                asin = self.extract_asin_from_url(original_link)
                
                logger.info(f"üîç Processing website product {i}/{website_count} - ASIN: {asin}")
                
                # Get REAL product information using FIXED SerpApi method
                product_info = await self.get_real_product_info_serpapi(asin)
                
                website_product = {
                    'id': f"product_{session_type}_{i}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    'title': product_info['title'],
                    'image': product_info['image'],
                    'affiliate_link': converted_link,
                    'price': product_info['price'],
                    'rating': product_info['rating'],
                    'category': product_info['category'],
                    'asin': product_info['asin'],
                    'data_source': product_info.get('source', 'unknown'),
                    'posted_date': datetime.now(pytz.timezone('Asia/Kolkata')).isoformat(),
                    'session_type': session_type,
                    'link_index': self.current_index - website_count + i - 1
                }
                
                website_products.append(website_product)
                
                logger.info(f"‚úÖ Website product {i}/{website_count} processed successfully!")
                logger.info(f"   üìù Title: {product_info['title'][:50]}...")
                logger.info(f"   üí∞ Price: {product_info['price']}")
                logger.info(f"   ‚≠ê Rating: {product_info['rating']}")
                logger.info(f"   üìä Data source: {product_info.get('source', 'unknown')}")
                
                # Delay between product processing to avoid rate limits
                if i < len(website_links):
                    await asyncio.sleep(5)
                    
            except Exception as e:
                logger.error(f"‚ùå Error processing website product {i}: {e}")
                logger.error(f"üìã Full error: {traceback.format_exc()}")
        
        # Update website
        if website_products:
            logger.info(f"üåê Updating website with {len(website_products)} products...")
            website_success = await self.update_website_products(website_products)
            if website_success:
                logger.info(f"üéâ Website successfully updated!")
            else:
                logger.error("‚ùå Website update failed")
        else:
            logger.warning("‚ö†Ô∏è No products to update on website")
        
        # Final comprehensive summary
        total_links = len(self.links) if self.links else 0
        cycle_number = (self.current_index // total_links) if total_links > 0 else 1
        remaining_in_cycle = total_links - (self.current_index % total_links) if total_links > 0 else 0
        
        logger.info(f"")
        logger.info(f"üéâ {session_type.upper()} SESSION COMPLETE! üéâ")
        logger.info(f"   üì± Telegram: {sent_count}/{telegram_count} links posted")
        logger.info(f"   üåê Website: {len(website_products)}/{website_count} products updated")
        logger.info(f"   üìç Progress: {self.current_index}/{total_links}")
        logger.info(f"   üîÑ Cycle: {cycle_number}, Remaining: {remaining_in_cycle}")
        logger.info(f"   üîë SerpApi requests used: {self.serp_request_count}/250")
        logger.info(f"   üìä Real data fetched: {sum(1 for p in website_products if p.get('data_source') == 'serpapi')}/{len(website_products)}")
        logger.info(f"")
        
        return sent_count

# Main execution function
async def main():
    """Main function for scheduled execution"""
    logger.info("üöÄ Starting Enhanced Telegram + Website Bot with SerpApi...")
    logger.info(f"üìÖ Session Type: {SESSION_TYPE.upper()}")
    logger.info(f"üïê IST Time: {datetime.now(pytz.timezone('Asia/Kolkata')).strftime('%Y-%m-%d %I:%M:%S %p')}")
    
    # Validate required environment variables
    required_vars = {
        'BOT_TOKEN': BOT_TOKEN,
        'CHANNEL_ID': CHANNEL_ID,
        'AMAZON_TAG': AMAZON_AFFILIATE_TAG,
        'GITHUB_TOKEN': GITHUB_TOKEN
    }
    
    missing_vars = [name for name, value in required_vars.items() if not value]
    if missing_vars:
        logger.error(f"‚ùå Missing required environment variables: {', '.join(missing_vars)}")
        exit(1)
    
    if not SERP_API_KEY:
        logger.warning("‚ö†Ô∏è No SerpApi key found - will use enhanced fallback product data")
    else:
        logger.info(f"‚úÖ SerpApi key configured: fb21165a43...{SERP_API_KEY[-4:]}")
    
    # Initialize bot
    bot_instance = EnhancedAffiliateBot()
    
    logger.info(f"üìä Configuration Summary:")
    logger.info(f"   üì± Total links loaded: {len(bot_instance.links)}")
    logger.info(f"   üìç Starting from index: {bot_instance.current_index}")
    logger.info(f"   üéØ Target channel: {CHANNEL_ID}")
    logger.info(f"   üåê Target website: {WEBSITE_REPO}")
    logger.info(f"   üè∑Ô∏è Affiliate tag: {AMAZON_AFFILIATE_TAG}")
    
    try:
        # Execute the session
        sent_count = await bot_instance.send_scheduled_links(SESSION_TYPE)
        
        logger.info(f"")
        logger.info(f"üèÜ MISSION ACCOMPLISHED! üèÜ")
        logger.info(f"Session '{SESSION_TYPE}' completed successfully!")
        logger.info(f"Telegram posts: {sent_count}, Website: Updated with real data!")
        logger.info(f"")
        
    except Exception as e:
        logger.error(f"‚ùå Critical error in session: {e}")
        logger.error(f"üìã Full traceback: {traceback.format_exc()}")
        exit(1)

if __name__ == '__main__':
    asyncio.run(main())
